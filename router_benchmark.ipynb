{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router Benchmark on PYNQ-Z2\n",
    "\n",
    "This notebook demonstrates how to run the router benchmark and read cycle counts from the FPGA.\n",
    "\n",
    "**LED Mapping:**\n",
    "- LED0 = Base12 (condition 2)\n",
    "- LED1 = Router (condition 3) \u2014 should light with default latencies\n",
    "- LED2 = Base10 (condition 1)\n",
    "- LED3 = Base2 (condition 0)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Copy `router_bench.bit` and `router_bench.hwh` to the same directory as this notebook\n",
    "- Or update the `overlay_path` below to point to the correct location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq import Overlay, MMIO\n",
    "import time\n",
    "\n",
    "# Load the overlay\n",
    "overlay_path = \"router_bench.bit\"\n",
    "ol = Overlay(overlay_path)\n",
    "\n",
    "print(\"Overlay loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MMIO object for the benchmark module\n",
    "BASE_ADDR = 0x43C00000\n",
    "mmio = MMIO(BASE_ADDR, 0x1000)\n",
    "\n",
    "# Register offsets\n",
    "CTRL     = 0x00  # Control: bit0=start, bit1=soft_clear\n",
    "STAT     = 0x04  # Status: bit0=running, bit1=done, bits[3:2]=winner_code\n",
    "T0       = 0x08  # Cycle count for condition 0 (Base2)\n",
    "T1       = 0x0C  # Cycle count for condition 1 (Base10)\n",
    "T2       = 0x10  # Cycle count for condition 2 (Base12)\n",
    "T3       = 0x14  # Cycle count for condition 3 (Router)\n",
    "ONEHOT   = 0x18  # Internal one-hot winner (cond0..3)\n",
    "TTOTAL   = 0x1C  # Sum of condition cycle totals\n",
    "TRUNTIME = 0x20  # Cycles from start\u2192done (includes control overhead)\n",
    "INFO     = 0x24  # [15:0] = operations executed per condition\n",
    "\n",
    "print(\"MMIO initialized at address 0x{:08X}\".format(BASE_ADDR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLK_FREQ_HZ = 100_000_000  # AXI clock frequency; update if different on your platform\n",
    "CLK_PERIOD_NS = 1e9 / CLK_FREQ_HZ\n",
    "\n",
    "def cycles_to_seconds(cycles):\n",
    "    return cycles / CLK_FREQ_HZ\n",
    "\n",
    "def cycles_to_microseconds(cycles):\n",
    "    return cycles_to_seconds(cycles) * 1e6\n",
    "\n",
    "def start_benchmark():\n",
    "    \"\"\"Start the benchmark by writing bit0=1 to CTRL register\"\"\"\n",
    "    mmio.write(CTRL, 0x1)\n",
    "\n",
    "def read_status():\n",
    "    \"\"\"Read status register and decode fields\"\"\"\n",
    "    v = mmio.read(STAT)\n",
    "    running     = (v & 0x1) != 0\n",
    "    done        = (v & 0x2) != 0\n",
    "    winner_code = (v >> 2) & 0x3\n",
    "    return running, done, winner_code\n",
    "\n",
    "def wait_done(timeout_s=2.0):\n",
    "    \"\"\"Wait for benchmark to complete with timeout\"\"\"\n",
    "    t0 = time.time()\n",
    "    while True:\n",
    "        running, done, winner_code = read_status()\n",
    "        if done:\n",
    "            return winner_code\n",
    "        if (time.time() - t0) > timeout_s:\n",
    "            raise TimeoutError(\"Benchmark did not finish\")\n",
    "        time.sleep(0.001)\n",
    "\n",
    "def read_cycles():\n",
    "    \"\"\"Read cycle counts for all four conditions\"\"\"\n",
    "    return (mmio.read(T0), mmio.read(T1), mmio.read(T2), mmio.read(T3))\n",
    "\n",
    "def read_metrics():\n",
    "    \"\"\"Read cycle counts plus aggregate benchmark metrics\"\"\"\n",
    "    cycles = read_cycles()\n",
    "    total_cycles = mmio.read(TTOTAL)\n",
    "    runtime_cycles = mmio.read(TRUNTIME)\n",
    "    info = mmio.read(INFO)\n",
    "    ops_per_condition = info & 0xFFFF\n",
    "    return cycles, total_cycles, runtime_cycles, ops_per_condition\n",
    "\n",
    "print(\"Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition labels with LED mapping\n",
    "cond_labels = {\n",
    "    0: \"Base2   (LED3)\",\n",
    "    1: \"Base10  (LED2)\",\n",
    "    2: \"Base12  (LED0)\",\n",
    "    3: \"Router  (LED1)\",\n",
    "}\n",
    "\n",
    "print(\"Running benchmark...\")\n",
    "start_benchmark()\n",
    "winner = wait_done()\n",
    "(cycles, total_cycles, runtime_cycles, ops_per_condition) = read_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Benchmark Results\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nCycle totals per condition:\")\n",
    "for idx in range(len(cycles)):\n",
    "    label = cond_labels[idx]\n",
    "    cyc = cycles[idx]\n",
    "    time_us = cycles_to_microseconds(cyc)\n",
    "    cycles_per_op = cyc / ops_per_condition if ops_per_condition else float('nan')\n",
    "    print(f\"  {label:>14}: {cyc:6d} cycles ({time_us:8.3f} \u00b5s, {cycles_per_op:5.2f} cycles/op)\")\n",
    "\n",
    "total_time_us = cycles_to_microseconds(total_cycles)\n",
    "runtime_time_us = cycles_to_microseconds(runtime_cycles)\n",
    "total_ops = ops_per_condition * len(cycles)\n",
    "\n",
    "print(\"\\nAggregate metrics:\")\n",
    "print(f\"  Total benchmark cycles (sum of conditions): {total_cycles:6d} cycles ({total_time_us:8.3f} \u00b5s)\")\n",
    "print(f\"  Runtime cycles (start\u2192done):               {runtime_cycles:6d} cycles ({runtime_time_us:8.3f} \u00b5s)\")\n",
    "print(f\"  Operations executed: {ops_per_condition} per condition ({total_ops} total per run)\")\n",
    "if runtime_cycles:\n",
    "    throughput_ops_per_s = total_ops / cycles_to_seconds(runtime_cycles)\n",
    "    print(f\"  Effective throughput: {throughput_ops_per_s/1e3:8.2f} kOps/s\")\n",
    "\n",
    "print(f\"\\nWinner: cond{winner} \u2192 {cond_labels[winner]}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run multiple times and average\n",
    "import numpy as np\n",
    "\n",
    "num_runs = 10\n",
    "cycle_runs = []\n",
    "total_runs = []\n",
    "runtime_runs = []\n",
    "winner_history = []\n",
    "ops_per_condition = None\n",
    "\n",
    "print(f\"Running benchmark {num_runs} times...\")\n",
    "for i in range(num_runs):\n",
    "    start_benchmark()\n",
    "    winner = wait_done()\n",
    "    cycles, total_cycles, runtime_cycles, ops_val = read_metrics()\n",
    "    cycle_runs.append(cycles)\n",
    "    total_runs.append(total_cycles)\n",
    "    runtime_runs.append(runtime_cycles)\n",
    "    winner_history.append(winner)\n",
    "    if ops_per_condition is None:\n",
    "        ops_per_condition = ops_val\n",
    "    elif ops_per_condition != ops_val:\n",
    "        print(f\"Warning: ops_per_condition changed from {ops_per_condition} to {ops_val}\")\n",
    "        ops_per_condition = ops_val\n",
    "    print(f\"Run {i+1}: Base2={cycles[0]}, Base10={cycles[1]}, Base12={cycles[2]}, Router={cycles[3]}, Total={total_cycles}, Runtime={runtime_cycles} \u2192 Winner: cond{winner}\")\n",
    "\n",
    "cycle_runs = np.array(cycle_runs, dtype=np.float64)\n",
    "avg = np.mean(cycle_runs, axis=0)\n",
    "std = np.std(cycle_runs, axis=0)\n",
    "avg_total = float(np.mean(total_runs))\n",
    "std_total = float(np.std(total_runs))\n",
    "avg_runtime = float(np.mean(runtime_runs))\n",
    "std_runtime = float(np.std(runtime_runs))\n",
    "avg_time_us = avg / CLK_FREQ_HZ * 1e6\n",
    "std_time_us = std / CLK_FREQ_HZ * 1e6\n",
    "avg_total_time_us = avg_total / CLK_FREQ_HZ * 1e6\n",
    "std_total_time_us = std_total / CLK_FREQ_HZ * 1e6\n",
    "avg_runtime_time_us = avg_runtime / CLK_FREQ_HZ * 1e6\n",
    "std_runtime_time_us = std_runtime / CLK_FREQ_HZ * 1e6\n",
    "avg_cycles_per_op = avg / ops_per_condition if ops_per_condition else np.full_like(avg, np.nan)\n",
    "std_cycles_per_op = std / ops_per_condition if ops_per_condition else np.full_like(std, np.nan)\n",
    "total_ops = ops_per_condition * cycle_runs.shape[1] if ops_per_condition else np.nan\n",
    "avg_throughput_ops = (total_ops / (avg_runtime / CLK_FREQ_HZ)) if (ops_per_condition and avg_runtime) else float('nan')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Average over {num_runs} runs:\")\n",
    "print(\"=\"*50)\n",
    "for idx in range(cycle_runs.shape[1]):\n",
    "    label = cond_labels[idx]\n",
    "    print(\n",
    "        f\"  {label:>14}: {avg[idx]:6.1f} \u00b1 {std[idx]:4.1f} cycles \"\n",
    "        f\"({avg_time_us[idx]:7.3f} \u00b1 {std_time_us[idx]:5.3f} \u00b5s, {avg_cycles_per_op[idx]:5.2f} \u00b1 {std_cycles_per_op[idx]:5.2f} cycles/op)\"\n",
    "    )\n",
    "print(\"=\"*50)\n",
    "print(f\"  Total cycles: {avg_total:6.1f} \u00b1 {std_total:4.1f} cycles ({avg_total_time_us:7.3f} \u00b1 {std_total_time_us:5.3f} \u00b5s)\")\n",
    "print(f\"  Runtime cycles (start\u2192done): {avg_runtime:6.1f} \u00b1 {std_runtime:4.1f} cycles ({avg_runtime_time_us:7.3f} \u00b1 {std_runtime_time_us:5.3f} \u00b5s)\")\n",
    "print(f\"  Operations per condition: {ops_per_condition} \u2192 {total_ops} total operations per run\")\n",
    "if not np.isnan(avg_throughput_ops):\n",
    "    print(f\"  Average throughput: {avg_throughput_ops/1e3:8.2f} kOps/s\")\n",
    "\n",
    "winner_counts = np.bincount(winner_history, minlength=len(cond_labels))\n",
    "print(\"\\nWinner distribution:\")\n",
    "for idx in range(len(cond_labels)):\n",
    "    print(f\"  cond{idx} ({cond_labels[idx]}): {winner_counts[idx]} wins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conditions = ['Base2\\n(cond0)', 'Base10\\n(cond1)', 'Base12\\n(cond2)', 'Router\\n(cond3)']\n",
    "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(conditions, avg, yerr=std, color=colors, alpha=0.7, capsize=5)\n",
    "plt.ylabel('Cycles per condition', fontsize=12)\n",
    "plt.title('Router Benchmark Results (Average over {} runs)'.format(num_runs), fontsize=14)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, cyc_per_op, time_us in zip(bars, avg_cycles_per_op, avg_time_us):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height(),\n",
    "        f\"{cyc_per_op:5.2f} cyc/op\\n{time_us:6.3f} \u00b5s\",\n",
    "        ha='center', va='bottom', fontsize=9\n",
    "    )\n",
    "\n",
    "# Highlight the winner\n",
    "min_idx = int(np.argmin(avg))\n",
    "bars[min_idx].set_edgecolor('red')\n",
    "bars[min_idx].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Winner: {conditions[min_idx].replace(chr(10), ' ')} with {avg[min_idx]:.1f} cycles \"\n",
    "    f\"({avg_time_us[min_idx]:.3f} \u00b5s)\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}